{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CE807_Lab1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNGrLY64y3yGy1UVxZVagYB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ah20776/CE807/blob/main/Lab1/CE807_Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljv4EjjWNYDr"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "*******************************************************************************\n",
        "********* CE807 - Laboratory Sheet 1, Text pre-processing with Python *********\n",
        "*******************************************************************************\n",
        "\n",
        "Fixed by Ansgar Scherp.\n",
        "\n",
        "Changes made:\n",
        "1) Adapted to run under Python 3.\n",
        "2) Fixed missing variable declaration for English Snowball stemmer.\n",
        "\n",
        "Basic script from Massimo Poesio, edited by Dimitrios Andreou.\n",
        "\n",
        "Changes made:\n",
        "1) Included direct nltk downloads for missing modules within a try-except.\n",
        "2) Added print statements so that the the script produces output.\n",
        "3) Changed strings into Python3 strings, i.e. unicode.\n",
        "4) Removed recurring imports.\n",
        "\n",
        "Changes that could be made if intended to run as a script:\n",
        "1) Re-factor and organize multiple imports and variables.\n",
        "2) Could use pylab instead of matplotlib (ease of plotting).\n",
        "\n",
        "********************************* RUN SCRIPT AS *******************************\n",
        "python3 CE807-Lab1-20190117.py \n",
        "********************************* SCRIPT OUTPUT *******************************\n",
        "['disk', 'format', 'hard', 'how', 'my', 'problems', 'to']\n",
        "[[1 1 1 1 1 0 1]\n",
        " [1 1 1 0 0 1 0]]\n",
        "[1 1 1 1 1 0 1]\n",
        "1\n",
        "4690\n",
        "35788\n",
        "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst']\n",
        "cat\n",
        "[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('different', 'JJ')]\n",
        "['john', 'bought', 'carrots', 'potatoes']\n",
        "john\n",
        "bought\n",
        "carrot\n",
        "potato\n",
        "26888\n",
        "*******************************************************************************\n",
        "\"\"\"\n",
        "\n",
        "# Python3 version of strings, defaults to unicode.\n",
        "from __future__ import unicode_literals\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = np.asarray([[1,2],[2,3],[3,4],[4,5],[5,6]])\n",
        "x, y = data[:,0], data[:,1]\n",
        "# Could introduce plt.ion()/ioff(), so that one could see\n",
        "# interactively what is happening (also does not stop the\n",
        "# execution of the code if plt.draw() is called and not plt.show()).\n",
        "plt.scatter(x,y)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(min_df=1)\n",
        "content = ['How to format my hard disk', ' Hard disk format problems ']\n",
        "X = vectorizer.fit_transform(content)\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.toarray())\n",
        "print(X.toarray()[0])\n",
        "print(X.toarray()[1,2])\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = ['alt.atheism', 'soc.religion.christian',\n",
        "              'comp.graphics', 'sci.med']\n",
        "twenty_train = fetch_20newsgroups(subset='train', categories=categories,\n",
        "                                   shuffle=True, random_state=42)\n",
        "vectorizer = CountVectorizer()\n",
        "train_counts = vectorizer.fit_transform(twenty_train.data)\n",
        "# A\"u\" is placed before quotes, of any type (','',''',\",\"\",\"\"\",'\",...),\n",
        "# to tell you that the string is unicode; Python 3 switched to using \n",
        "# unicode in strings (hence the __future__ import).\n",
        "# Here is some further info (section 3.3): http://www.nltk.org/book/ch03.html\n",
        "# And if not enough here is something bigger from the CEO of StackOverflow:\n",
        "# https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/\n",
        "print(vectorizer.vocabulary_.get('algorithm'))\n",
        "print(len(vectorizer.get_feature_names()))\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "print(sorted(vectorizer.get_stop_words())[:20])\n",
        "\n",
        "import nltk\n",
        "\n",
        "s = nltk.stem.SnowballStemmer('english')\n",
        "print(s.stem('cats'))\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "try:\n",
        "    text = word_tokenize('And now for something completely different')\n",
        "except(LookupError):\n",
        "    print('Installing punkt.')\n",
        "    nltk.download('punkt')\n",
        "    text = word_tokenize('And now for something completely different')\n",
        "\n",
        "try:\n",
        "    print(nltk.pos_tag(text))\n",
        "except(LookupError):\n",
        "    print('Installing dependencies and maxent_treebank_pos_tagger.')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    nltk.download('maxent_treebank_pos_tagger')\n",
        "    print(nltk.pos_tag(text))\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "analyze = vectorizer.build_analyzer()\n",
        "print(analyze('John bought carrots and potatoes'))\n",
        "\n",
        "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
        "\n",
        "class StemmedCountVectorizer(CountVectorizer):\n",
        "    def build_analyzer(self):\n",
        "        analyzer = super(StemmedCountVectorizer,self).build_analyzer()\n",
        "        # Both \"()\" and \"[]\" work in the lambda statement below; the former\n",
        "        # creates a generator object while the latter a list i.e., one can\n",
        "        # be printed using \"print XXX\" while the other needs to access\n",
        "        # each element using a loop, as you have done in {*1} below,\n",
        "        # \"for i in XXX:print i\".\n",
        "        #\n",
        "\t# The essential difference lies in the fact that the latter method\n",
        "\t# will construct the whole list in the memory, whereas with a\n",
        "\t# generator, one is more 'lazy' and constructs only one member of the\n",
        "\t# list at a time, thus saving memory.\n",
        "\t#\n",
        "\t# Obviously if the generator is to be used multiple times,\n",
        "\t# constructing the list and keeping it in memory will be more\n",
        "\t# efficient.\n",
        "        return lambda doc:(english_stemmer.stem(w) for w in analyzer(doc))\n",
        "\n",
        "stem_vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n",
        "stem_analyze = stem_vectorizer.build_analyzer()\n",
        "Y = stem_analyze('John bought carrots and potatoes')\n",
        "\n",
        "for tok in Y: # {*1}\n",
        "    print(tok)\n",
        "\n",
        "train_counts = stem_vectorizer.fit_transform(twenty_train.data)\n",
        "print(len(stem_vectorizer.get_feature_names()))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}